{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Customer Data Cleaning ---\n",
    "# Load customer data\n",
    "customer_df = pd.read_excel(\"C:/Users/santh/Downloads/Customers.xlsx\")\n",
    "\n",
    "# Fill missing StateCode with 'Unknown'\n",
    "customer_df[\"StateCode\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Drop duplicate rows\n",
    "customer_df = customer_df.drop_duplicates()\n",
    "\n",
    "# Rename columns for consistency\n",
    "customer_df.rename(columns={\"Zip Code\": \"ZipCode\", \"State Code\": \"StateCode\"}, inplace=True)\n",
    "\n",
    "# Convert 'Birthday' column to datetime format and standardize to 'YYYY-MM-DD'\n",
    "customer_df['Birthday'] = pd.to_datetime(customer_df['Birthday'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Display customer DataFrame info and check for missing values\n",
    "customer_df.info()\n",
    "customer_df.isnull().sum()\n",
    "\n",
    "# Save cleaned customer data to CSV\n",
    "customer_df.to_csv(\"customers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exchange Rate Data Cleaning ---\n",
    "# Load exchange rate data\n",
    "exchange_df = pd.read_excel(\"C:/Users/santh/Downloads/Exchange_Rates.xlsx\")\n",
    "\n",
    "# Drop duplicate rows in the exchange rates DataFrame\n",
    "exchange_df = exchange_df.drop_duplicates()\n",
    "\n",
    "# Check for missing values\n",
    "exchange_df.info()\n",
    "exchange_df.isna().sum()\n",
    "\n",
    "# Rename columns for consistency\n",
    "exchange_df.rename(columns={\"Currency\": \"CurrencyCode\"}, inplace=True)\n",
    "\n",
    "# Save cleaned exchange rate data to CSV\n",
    "exchange_df.to_csv(\"exchange.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Product Data Cleaning ---\n",
    "# Load product data\n",
    "products_df = pd.read_excel(\"C:/Users/santh/Downloads/Products.xlsx\")\n",
    "\n",
    "# Drop duplicate rows in the product DataFrame\n",
    "products_df = products_df.drop_duplicates()\n",
    "\n",
    "# Rename columns for consistency\n",
    "products_df.rename(columns={\n",
    "    \"Product Name\": \"ProductName\",\n",
    "    \"Unit Cost USD\": \"UnitCostUSD\",\n",
    "    \"Unit Price USD\": \"UnitPriceUSD\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "products_df.isna().sum()\n",
    "\n",
    "# Save cleaned product data to CSV\n",
    "products_df.to_csv(\"products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Store Data Cleaning ---\n",
    "# Load store data\n",
    "stores_df = pd.read_excel(\"C:/Users/santh/Downloads/Stores.xlsx\")\n",
    "\n",
    "# Fill missing values in 'Square Meters' with the mean\n",
    "stores_df[\"Square Meters\"].fillna(stores_df[\"Square Meters\"].mean(), inplace=True)\n",
    "\n",
    "# Rename columns for consistency\n",
    "stores_df.rename(columns={\"Square Meters\": \"SquareMeters\", \"Open Date\": \"OpenDate\"}, inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "stores_df.isnull().sum()\n",
    "\n",
    "# Save cleaned store data to CSV\n",
    "stores_df.to_csv(\"stores.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sales Data Cleaning ---\n",
    "# Load sales data\n",
    "sales_df = pd.read_excel(\"C:/Users/santh/Downloads/Sales.xlsx\")\n",
    "\n",
    "# Convert 'Order Date' and 'Delivery Date' to datetime format\n",
    "sales_df[\"Order Date\"] = pd.to_datetime(sales_df[\"Order Date\"], format='%m/%d/%y')\n",
    "sales_df[\"Delivery Date\"] = pd.to_datetime(sales_df[\"Delivery Date\"], format='%m/%d/%y', errors='coerce')\n",
    "\n",
    "# Fill missing 'Delivery Date' values with 'Order Date' + average delivery time of 7 days\n",
    "average_delivery_time = 7\n",
    "sales_df['Delivery Date'].fillna(sales_df['Order Date'] + pd.to_timedelta(average_delivery_time, unit='D'), inplace=True)\n",
    "\n",
    "# Check for missing values in sales DataFrame\n",
    "sales_df.isna().sum()\n",
    "\n",
    "# Extract only the date part from 'Delivery Date'\n",
    "sales_df['Delivery Date'] = sales_df['Delivery Date'].dt.date\n",
    "\n",
    "# Rename columns for consistency\n",
    "sales_df.rename(columns={\n",
    "    \"Order Number\": \"OrderNumber\",\n",
    "    \"Line Item\": \"LineItem\",\n",
    "    \"Order Date\": \"OrderDate\",\n",
    "    \"Delivery Date\": \"DeliveryDate\",\n",
    "    \"Currency Code\": \"CurrencyCode\"\n",
    "}, inplace=True)\n",
    "\n",
    "# Save cleaned sales data to CSV\n",
    "sales_df.to_csv(\"sales.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Merging ---\n",
    "# Merge sales, customer, store, and product data\n",
    "merged_df = sales_df.merge(customer_df, on='CustomerKey', how='inner') \\\n",
    "    .merge(stores_df, on='StoreKey', how='inner') \\\n",
    "    .merge(products_df, on='ProductKey', how='inner')\n",
    "\n",
    "# Check merged DataFrame info and missing values\n",
    "merged_df.info()\n",
    "merged_df.isna().sum()\n",
    "\n",
    "# Save the merged DataFrame to a CSV file\n",
    "merged_df.to_csv(\"merged_data.csv\", index=False)\n",
    "\n",
    "# --- Additional Exchange Rate Merging ---\n",
    "# Drop duplicates from exchange DataFrame based on specific columns\n",
    "exchange_df.drop_duplicates(subset=[\"Date\", \"CurrencyCode\", \"Exchange\"], inplace=True)\n",
    "\n",
    "# Merge exchange rate data with the merged DataFrame\n",
    "new_df = merged_df.merge(exchange_df, on=\"CurrencyCode\", how=\"inner\")\n",
    "\n",
    "# Final output of merged DataFrame\n",
    "new_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Step 1: Load your multiple cleaned datasets\n",
    "merged_data = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/merged_data.csv\")  # First dataset\n",
    "customers = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/customers.csv\")  # Second dataset\n",
    "products = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/products.csv\") \n",
    "stores = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/stores.csv\") \n",
    "sales = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/sales.csv\") \n",
    "exchange = pd.read_csv(\"C:/Users/santh/OneDrive/Documents/Guvi/Visual Studio/Python/exchange.csv\") \n",
    "\n",
    "# Step 2: Connect to MySQL using SQLAlchemy\n",
    "# Replace USERNAME, PASSWORD, HOST, and DATABASE with your MySQL credentials\n",
    "engine = create_engine('mysql+pymysql://root:mysql4505@127.0.0.1/globaltech')\n",
    "\n",
    "# Step 3: Insert DataFrames into MySQL\n",
    "# Specify the table names you want to insert data into\n",
    "table_names = {\n",
    "    'merged_data': merged_data,\n",
    "    'customers': customers,\n",
    "    'products': products,\n",
    "    'stores': stores,\n",
    "    'sales': sales,\n",
    "    'exchange': exchange,\n",
    "}\n",
    "\n",
    "# Loop through the table names and insert each DataFrame\n",
    "for table_name, df in table_names.items():\n",
    "    df.to_sql(table_name, con=engine, index=False, if_exists='replace')  # Change 'replace' to 'append' if you want to keep existing data\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
